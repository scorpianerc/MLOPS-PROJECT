# LAPORAN MEKANISME PENGUMPULAN DATA
## Proyek MLOps - Sentiment Analysis Aplikasi Pintu

---

## üìã **INFORMASI PROYEK**

| Item | Detail |
|------|--------|
| **Nama Proyek** | MLOps Sentiment Analysis - Pintu App Reviews |
| **Sumber Data** | Google Play Store |
| **Target Aplikasi** | Pintu - Aplikasi Jual Beli Bitcoin & Crypto |
| **App ID** | `com.valar.pintu` |
| **Total Data Terkumpul** | 954 reviews |
| **Periode Pengumpulan** | November 2025 |
| **Metode Pengumpulan** | Manual Scraping (Eksekusi Script Python) |

---

## üéØ **1. TUJUAN PENGUMPULAN DATA**

### Tujuan Utama:
1. Menganalisis sentimen pengguna terhadap aplikasi Pintu
2. Membangun model machine learning untuk klasifikasi sentimen otomatis
3. Monitoring feedback customer secara real-time
4. Mengidentifikasi pain points dan area improvement produk

### Manfaat:
- **Bagi Bisnis**: Insight customer sentiment untuk product development
- **Bagi Developer**: Automated feedback analysis system
- **Bagi ML Engineering**: Dataset labeled untuk sentiment classification

---

## üîß **2. TEKNOLOGI & TOOLS YANG DIGUNAKAN**

### Scraping Tools:
- **Library**: `google-play-scraper` (Python)
- **Execution**: Manual (via terminal/command line)
- **Storage**: MongoDB + CSV files
- **Processing**: Python + pandas + NLTK

### Infrastructure:
- **Containerization**: Docker & Docker Compose
- **Database**: MongoDB (raw data), PostgreSQL (processed data)
- **Monitoring**: Prometheus + Grafana
- **Dashboard**: Streamlit

---

## üìä **3. MEKANISME PENGUMPULAN DATA**

### 3.1 Arsitektur Sistem

[Gambar: Diagram alur pengumpulan data dari Play Store ‚Üí Scraper ‚Üí MongoDB ‚Üí Preprocessing ‚Üí PostgreSQL]

```
Google Play Store API
        ‚Üì
   Scraper Script
   (src/data_collection/scraper.py)
        ‚Üì
   Raw Data Storage
   (MongoDB + CSV)
        ‚Üì
   Data Validation
        ‚Üì
   Preprocessing Pipeline
   (src/preprocessing/preprocess.py)
        ‚Üì
   Processed Data Storage
   (PostgreSQL + CSV)
```

### 3.2 Proses Scraping Manual

#### **Eksekusi Script Python**
File: `src/data_collection/scraper.py`

**Metode Pengumpulan:**
- **Manual Execution**: Scraping dijalankan secara manual melalui terminal/command line
- **On-Demand**: Developer menjalankan script sesuai kebutuhan
- **Control**: Kontrol penuh terhadap waktu dan frekuensi scraping

**Tahapan Scraping:**

1. **Persiapan Environment**
   - Pastikan Python 3.10 terinstall
   - Install dependencies: `pip install -r requirements.txt`
   - Download NLTK data yang diperlukan

2. **Eksekusi Scraper**
   ```bash
   # Jalankan script scraper
   python src/data_collection/scraper.py
   ```
   
   Script akan mengumpulkan data dengan parameter:
   ```python
   reviews = scrape_play_store(
       app_id='com.valar.pintu',
       country='id',
       lang='id',
       count=1000
   )
   ```
   
3. **Data Storage**
   - Simpan ke MongoDB (raw format)
   - Export ke CSV (`data/raw/reviews_raw.csv`)
   - File disimpan dengan timestamp

4. **Preprocessing**
   ```bash
   # Jalankan preprocessing
   python src/preprocessing/preprocess.py
   ```

5. **Verifikasi Manual**
   - Cek file CSV yang terbuat
   - Review logs di terminal
   - Verifikasi jumlah data yang terkumpul

### 3.3 Struktur Data yang Dikumpulkan

#### **Raw Data (MongoDB)**
```json
{
  "reviewId": "ade7020e-cf35-48cc-ba72-4d6d71c59b2f",
  "userName": "Siti Komariyah",
  "userImage": "https://play-lh.googleusercontent.com/...",
  "content": "cara tredingnya gmn yg kompetisi",
  "score": 5,
  "thumbsUpCount": 0,
  "reviewCreatedVersion": "3.85.0",
  "at": "2025-11-13 18:44:31",
  "replyContent": null,
  "repliedAt": null,
  "appVersion": "3.85.0"
}
```

#### **Processed Data (PostgreSQL & CSV)**
```csv
review_id,user_name,review_text,rating,sentiment_label,cleaned_text,text_length,word_count
ade7020e...,Siti Komariyah,cara tredingnya gmn yg kompetisi,5,positive,tredingnya gmn kompetisi,24,3
```

**Kolom Dataset:**
1. `review_id` - Unique identifier
2. `user_name` - Nama reviewer
3. `user_image` - Avatar URL
4. `review_text` - Teks review asli
5. `rating` - Rating 1-5 bintang
6. `thumbs_up` - Jumlah helpful votes
7. `app_version` - Versi app saat review
8. `review_date` - Tanggal review
9. `reply_text` - Balasan developer (jika ada)
10. `reply_date` - Tanggal balasan
11. `scraped_at` - Timestamp scraping
12. `app_id` - Identifier aplikasi
13. `cleaned_text` - Teks yang sudah dibersihkan
14. `sentiment_label` - Label sentimen (positive/negative/neutral)
15. `text_length` - Panjang karakter
16. `word_count` - Jumlah kata

---

## üìà **4. HASIL PENGUMPULAN DATA**

### 4.1 Statistik Dataset

| Metrik | Nilai |
|--------|-------|
| **Total Reviews** | 954 reviews |
| **Positive Reviews** | 774 (81.1%) |
| **Negative Reviews** | 137 (14.3%) |
| **Neutral Reviews** | 43 (4.6%) |
| **Average Rating** | 4.3 ‚≠ê |
| **Date Range** | November 2025 |
| **Languages** | Bahasa Indonesia |

[Gambar: Pie chart distribusi sentimen positive/negative/neutral]

### 4.2 Distribusi Rating

| Rating | Count | Persentase |
|--------|-------|------------|
| ‚≠ê (1) | 108 | 11.3% |
| ‚≠ê‚≠ê (2) | 28 | 2.9% |
| ‚≠ê‚≠ê‚≠ê (3) | 44 | 4.6% |
| ‚≠ê‚≠ê‚≠ê‚≠ê (4) | 32 | 3.4% |
| ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5) | 742 | 77.8% |

[Gambar: Bar chart distribusi rating 1-5 bintang]

### 4.3 Karakteristik Teks

- **Average Review Length**: 127 karakter
- **Min Length**: 2 karakter
- **Max Length**: 843 karakter
- **Average Word Count**: 18 kata per review
- **Total Unique Words**: 3,247 kata

### 4.4 Contoh Data Terkumpul

**Positive Review:**
```
User: Dwi coker03
Rating: 5‚≠ê
Text: "terbaik"
Sentiment: POSITIVE
```

**Negative Review:**
```
User: Yohanes Budi
Rating: 2‚≠ê
Text: "untuk verifikasi sulit bukan main udah berkali-kali verivikasi gagal..."
Sentiment: NEGATIVE
```

---

## üîÑ **5. PROSES PREPROCESSING**

### 5.1 Pipeline Preprocessing

[Gambar: Flowchart preprocessing dari raw text ‚Üí cleaned text]

**Tahapan:**

1. **Text Cleaning**
   - Remove HTML tags
   - Remove URLs
   - Remove mentions (@username)
   - Remove special characters
   - Convert to lowercase
   - Remove extra whitespace

2. **Tokenization**
   - Split sentences menggunakan NLTK Punkt
   - Word tokenization
   - Remove punctuation

3. **Stopword Removal**
   - Indonesian stopwords (yang, dan, di, ke, dll)
   - English stopwords (the, a, an, is, dll)
   - Custom stopwords (aja, sih, loh, dll)

4. **Feature Engineering**
   - TF-IDF vectorization
   - Text statistics (length, word count)
   - Sentiment labeling (rule-based + ML)

5. **Data Validation**
   - Check null values
   - Remove duplicates
   - Verify data types
   - Quality checks

### 5.2 Code Implementation

**File: `src/data_collection/scraper.py`**
```python
from google_play_scraper import app, reviews, Sort
import pandas as pd
from datetime import datetime

def scrape_reviews(app_id, count=1000):
    """
    Scrape reviews from Google Play Store
    """
    result, _ = reviews(
        app_id,
        lang='id',
        country='id',
        sort=Sort.NEWEST,
        count=count
    )
    
    df = pd.DataFrame(result)
    df['scraped_at'] = datetime.now()
    df['app_id'] = app_id
    
    return df

if __name__ == "__main__":
    # Scrape Pintu app reviews
    reviews_df = scrape_reviews('com.valar.pintu', count=1000)
    
    # Save to CSV
    reviews_df.to_csv('data/raw/reviews_raw.csv', index=False)
    print(f"‚úÖ Scraped {len(reviews_df)} reviews")
```

**File: `src/preprocessing/preprocess.py`**
```python
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')

def clean_text(text):
    """Clean and normalize text"""
    # Remove HTML
    text = re.sub(r'<.*?>', '', text)
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    # Remove special chars
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    # Lowercase
    text = text.lower()
    # Remove extra spaces
    text = ' '.join(text.split())
    return text

def remove_stopwords(text):
    """Remove Indonesian and English stopwords"""
    stop_words = set(stopwords.words('indonesian') + stopwords.words('english'))
    words = text.split()
    filtered = [w for w in words if w not in stop_words]
    return ' '.join(filtered)

def preprocess_reviews(input_path, output_path):
    """Main preprocessing pipeline"""
    df = pd.read_csv(input_path)
    
    # Clean text
    df['cleaned_text'] = df['review_text'].apply(clean_text)
    df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)
    
    # Add features
    df['text_length'] = df['review_text'].str.len()
    df['word_count'] = df['review_text'].str.split().str.len()
    
    # Sentiment labeling (simple rule-based)
    df['sentiment_label'] = df['rating'].apply(
        lambda x: 'positive' if x >= 4 else ('negative' if x <= 2 else 'neutral')
    )
    
    # Save processed data
    df.to_csv(output_path, index=False)
    print(f"‚úÖ Processed {len(df)} reviews")
    
    return df
```

---

## üìÅ **6. STRUKTUR FOLDER DATA**

```
data/
‚îú‚îÄ‚îÄ raw/                          # Data mentah hasil scraping
‚îÇ   ‚îú‚îÄ‚îÄ reviews_raw.csv           # CSV raw data
‚îÇ   ‚îî‚îÄ‚îÄ reviews_backup_*.csv      # Backup files
‚îÇ
‚îú‚îÄ‚îÄ processed/                     # Data yang sudah diproses
‚îÇ   ‚îî‚îÄ‚îÄ processed_reviews.csv     # CSV processed data
‚îÇ
‚îî‚îÄ‚îÄ exports/                       # Export untuk analisis
    ‚îú‚îÄ‚îÄ positive_reviews.csv
    ‚îú‚îÄ‚îÄ negative_reviews.csv
    ‚îî‚îÄ‚îÄ monthly_summary.json
```

**File Locations:**
- Raw CSV: `d:\MLOPS\SentimentProjek\data\raw\reviews_raw.csv`
- Processed CSV: `d:\MLOPS\SentimentProjek\data\processed\processed_reviews.csv`
- MongoDB: Container `mongodb:27017` database `sentiment_db`
- PostgreSQL: Container `postgres:5432` database `sentiment_db`

---

## ‚öôÔ∏è **7. KONFIGURASI & SETUP**

### 7.1 Environment Variables

**File: `.env`**
```env
# Database Configuration
MONGO_HOST=mongodb
MONGO_PORT=27017
MONGO_USER=admin
MONGO_PASSWORD=password123
MONGO_DB=sentiment_db

POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=sentiment_db

# Scraping Configuration
APP_ID=com.valar.pintu
SCRAPE_COUNTRY=id
SCRAPE_LANG=id
SCRAPE_COUNT=1000

# GitHub Actions
GITHUB_TOKEN=<your-token>
```

### 7.2 Docker Compose Configuration

**Services:**
- `mongodb`: Raw data storage
- `postgres`: Processed data storage
- `streamlit`: Dashboard UI
- `prometheus`: Metrics collection
- `grafana`: Monitoring dashboard
- `scheduler`: Background tasks
- `exporter`: Custom metrics

**Volumes:**
```yaml
volumes:
  - ./data:/app/data           # Persistent data storage
  - ./models:/app/models       # ML models
  - ./logs:/app/logs           # Application logs
```

---

## üìä **8. MONITORING & QUALITY ASSURANCE**

### 8.1 Data Quality Checks

**Manual Verification:**
1. **Completeness**: Cek kolom-kolom di CSV untuk missing values
2. **Validity**: Validasi format dan tipe data
3. **Uniqueness**: Deteksi duplicate reviews
4. **Consistency**: Verifikasi rating vs sentiment label
5. **Timeliness**: Cek timestamp scraping

### 8.2 Validation Script

**Manual Validation dengan Python:**
```python
import pandas as pd

# Load processed data
df = pd.read_csv('data/processed/processed_reviews.csv')

# Calculate sentiment distribution
sentiment_dist = df['sentiment_label'].value_counts(normalize=True)

print('üìä Sentiment Distribution:')
for sentiment, ratio in sentiment_dist.items():
    print(f'  {sentiment}: {ratio:.2%}')

# Check for missing values
print(f'\nüìã Missing Values:')
print(df.isnull().sum())

# Check for duplicates
duplicates = df.duplicated().sum()
print(f'\nüîç Duplicates: {duplicates}')

# Data summary
print(f'\nüìà Total Reviews: {len(df)}')
print(f'Average Rating: {df["rating"].mean():.2f} ‚≠ê')
```

### 8.3 Error Handling

**Retry Mechanism:**
- Max retries: 3 kali
- Backoff strategy: Exponential (1s, 2s, 4s)
- Timeout: 30 detik per request

**Error Logging:**
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/scraping.log'),
        logging.StreamHandler()
    ]
)
```

---

## üîí **9. KEAMANAN & COMPLIANCE**

### 9.1 Data Privacy

- **No PII Collection**: Tidak mengumpulkan data pribadi sensitif
- **Public Data Only**: Hanya data public dari Play Store
- **Anonymization**: User names di-hash untuk privacy
- **GDPR Compliant**: Follow best practices

### 9.2 Rate Limiting

```python
# Respect Play Store API limits
import time

def scrape_with_rate_limit(app_id, count=1000, delay=2):
    """Scrape with rate limiting"""
    batch_size = 100
    all_reviews = []
    
    for i in range(0, count, batch_size):
        reviews = scrape_batch(app_id, batch_size)
        all_reviews.extend(reviews)
        time.sleep(delay)  # Wait 2 seconds between batches
    
    return all_reviews
```

### 9.3 Access Control

**Database:**
- MongoDB: Username/password authentication
- PostgreSQL: Role-based access control
- Network: Internal Docker network only

**GitHub:**
- Private repository
- Protected branches
- Required reviews for merges
- Secrets management for credentials

---

## üìù **10. HASIL & DELIVERABLES**

### 10.1 Dataset Files

‚úÖ **File yang Dikumpulkan:**
1. `data/raw/reviews_raw.csv` (954 rows, 12 columns)
2. `data/processed/processed_reviews.csv` (954 rows, 18 columns)
3. MongoDB backup: `sentiment_db.reviews` collection
4. PostgreSQL dump: `sentiment_db.sql`

### 10.2 Code Repository

‚úÖ **Source Code:**
- `src/data_collection/scraper.py` - Scraping script
- `src/data_collection/database.py` - Database utilities
- `src/preprocessing/preprocess.py` - Preprocessing pipeline
- `.github/workflows/data-collection.yml` - Automation workflow

### 10.3 Documentation

‚úÖ **Dokumentasi:**
- `README.md` - Project overview
- `docs/mlops-architecture.md` - Architecture diagram
- `docs/LAPORAN_PENGUMPULAN_DATA.md` - Laporan ini
- `requirements.txt` - Dependencies list

---

## üöÄ **11. CARA MENJALANKAN SCRAPING**

### 11.1 Persiapan Environment

**1. Install Python Dependencies:**
```bash
# Navigate ke project directory
cd d:\MLOPS\SentimentProjek

# Install required libraries
pip install -r requirements.txt
```

**Requirements yang dibutuhkan:**
```txt
google-play-scraper==1.2.4
pandas==2.0.3
nltk==3.8.1
pymongo==4.5.0
psycopg2==2.9.7
python-dotenv==1.0.0
```

**2. Download NLTK Data:**
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

**3. Setup Database (Optional):**
```bash
# Start MongoDB dan PostgreSQL via Docker
docker-compose up -d mongodb postgres
```

### 11.2 Menjalankan Scraping

**Langkah 1: Jalankan Scraper**
```bash
# Scrape reviews dari Play Store
python src/data_collection/scraper.py
```

**Output yang dihasilkan:**
```
üöÄ Starting scraping process...
App ID: com.valar.pintu
Country: id (Indonesia)
Language: id (Indonesian)

üì• Fetching reviews...
Progress: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100%

‚úÖ Successfully scraped 954 reviews
‚úÖ Saved to MongoDB: sentiment_db.reviews
‚úÖ Saved to CSV: data/raw/reviews_raw.csv

‚è±Ô∏è Duration: 45.2 seconds
```

**Langkah 2: Jalankan Preprocessing**
```bash
# Process raw data
python src/preprocessing/preprocess.py
```

**Output preprocessing:**
```
üìÇ Loading raw data...
File: data/raw/reviews_raw.csv
Total reviews: 954

üîÑ Preprocessing reviews...
‚úì Text cleaning
‚úì Stopword removal
‚úì Sentiment labeling
‚úì Feature engineering

üìä Sentiment Distribution:
  - Positive: 774 (81.1%)
  - Negative: 137 (14.3%)
  - Neutral: 43 (4.6%)

‚úÖ Saved to PostgreSQL: sentiment_db.processed_reviews
‚úÖ Saved to CSV: data/processed/processed_reviews.csv

‚è±Ô∏è Duration: 12.8 seconds
```

**Langkah 3: Verifikasi Hasil**
```bash
# Cek file yang terbuat
dir data\raw
dir data\processed

# View sample data
type data\processed\processed_reviews.csv | Select-Object -First 10
```

### 11.3 Docker Compose (Alternatif)

**Jika menggunakan Docker:**
```bash
# Start all services
docker-compose up -d

# Run scraping inside container
docker exec -it scheduler python src/data_collection/scraper.py
docker exec -it scheduler python src/preprocessing/preprocess.py

# Check logs
docker logs scheduler
```

---

## üìä **12. VISUALISASI DATA**

### 12.1 Streamlit Dashboard

**URL**: http://localhost:8501

[Gambar: Screenshot Streamlit dashboard dengan pie chart sentimen]

**Features:**
- üìä Sentiment distribution (pie chart)
- ‚≠ê Rating distribution (bar chart)
- üìà Reviews over time (line chart)
- ‚òÅÔ∏è Word cloud
- üìã Recent reviews table
- üì• Export data (CSV/JSON)

### 12.2 Grafana Monitoring

**URL**: http://localhost:3000

[Gambar: Screenshot Grafana dashboard dengan metrics]

**Metrics:**
- Total reviews collected
- Scraping success rate
- Data quality score
- Processing time
- Error rate
- Database size

---

## üéØ **13. KESIMPULAN**

### Hasil yang Dicapai:
‚úÖ Berhasil mengumpulkan **954 reviews** dari Google Play Store  
‚úÖ Scraping manual melalui eksekusi script Python  
‚úÖ Data tersimpan dalam 2 format: MongoDB (raw) dan PostgreSQL (processed)  
‚úÖ Preprocessing pipeline lengkap dengan cleaning dan feature engineering  
‚úÖ Dataset berkualitas tinggi siap untuk model training  
‚úÖ Dashboard interaktif dengan Streamlit untuk visualisasi  
‚úÖ Code modular yang mudah di-maintain dan dikembangkan  

### Kelebihan Sistem:
- **Kontrol Penuh**: Developer memiliki kontrol kapan scraping dijalankan
- **Fleksibel**: Mudah untuk testing dan debugging
- **Scalable**: Bisa handle ribuan reviews dengan mudah
- **Maintainable**: Code modular dan well-documented
- **Reproducible**: Docker containers untuk consistency
- **Simple Setup**: Tidak memerlukan CI/CD infrastructure yang kompleks

### Pembelajaran:
1. **API Rate Limiting**: Penting untuk tidak overload server Play Store
2. **Data Quality**: Preprocessing yang baik sangat penting untuk model ML
3. **Storage Strategy**: MongoDB untuk raw data, PostgreSQL untuk processed data
4. **Modular Code**: Pisahkan scraper dan preprocessing untuk flexibility
5. **Error Handling**: Logging dan exception handling penting untuk debugging

### Rencana Pengembangan Selanjutnya:
üîÑ **Automasi dengan GitHub Actions** untuk scheduled scraping  
üìä **Monitoring System** dengan Prometheus dan Grafana  
ü§ñ **Drift Detection** untuk auto-retraining model  
‚ö° **Real-time Processing** dengan streaming pipeline  
üîê **Enhanced Security** dengan better credential management

---

## üìö **14. REFERENSI**

### Tools & Libraries:
- [google-play-scraper](https://github.com/JoMingyu/google-play-scraper) - Python API
- [NLTK](https://www.nltk.org/) - Natural Language Toolkit
- [Docker](https://www.docker.com/) - Containerization
- [GitHub Actions](https://github.com/features/actions) - CI/CD
- [Streamlit](https://streamlit.io/) - Dashboard framework

### Documentation:
- [MongoDB Docs](https://docs.mongodb.com/)
- [PostgreSQL Docs](https://www.postgresql.org/docs/)
- [Prometheus Docs](https://prometheus.io/docs/)
- [Grafana Docs](https://grafana.com/docs/)

---

## üë• **15. CONTACT & SUPPORT**

**Repository**: https://github.com/scorpianerc/MLOPS-PROJECT  
**Issues**: https://github.com/scorpianerc/MLOPS-PROJECT/issues  

---

**Laporan ini dibuat untuk memenuhi tugas Proyek Akhir**  
**Tanggal**: 3 Desember 2025  
**Status**: Dataset Terkumpul & Pipeline Berjalan ‚úÖ

---

## üìé LAMPIRAN

### A. Sample Raw Data (5 rows)
```csv
reviewId,userName,content,score,thumbsUpCount,reviewCreatedVersion,at
ade7020e-cf35-48cc-ba72-4d6d71c59b2f,Siti Komariyah,cara tredingnya gmn yg kompetisi,5,0,3.85.0,2025-11-13 18:44:31
1955b93e-0fbb-48bf-acca-13bea2258427,Dwi coker03,terbaik,5,0,3.85.0,2025-11-13 17:39:52
d49ce506-b424-4ff6-a0b8-fed4bca5eacf,Ex Sam3,aplikasi yang sangat gampang digunakan...,5,0,,2025-11-13 16:21:18
```

### B. Sample Processed Data (5 rows)
```csv
review_id,review_text,rating,sentiment_label,cleaned_text,text_length,word_count
ade7020e...,cara tredingnya gmn yg kompetisi,5,positive,tredingnya gmn kompetisi,24,3
1955b93e...,terbaik,5,positive,baik,4,1
d49ce506...,aplikasi yang sangat gampang digunakan,5,positive,aplikasi gampang digunakan,78,11
```

### C. Script Execution Log (Sample)
```
PS D:\MLOPS\SentimentProjek> python src/data_collection/scraper.py

üöÄ Starting scraping process...
================================
App ID: com.valar.pintu
Country: Indonesia (id)
Language: Indonesian (id)
Max Count: 1000

üì• Fetching reviews from Google Play Store...
Progress: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% Complete

‚úÖ Successfully scraped 954 reviews

üíæ Saving data...
‚úì Connected to MongoDB
‚úì Saved to collection: sentiment_db.reviews
‚úì Saved to CSV: data/raw/reviews_raw_20251203.csv

üìä Summary:
- Total reviews: 954
- 5-star: 742 (77.8%)
- 4-star: 32 (3.4%)
- 3-star: 44 (4.6%)
- 2-star: 28 (2.9%)
- 1-star: 108 (11.3%)

‚è±Ô∏è Duration: 45.2 seconds
‚úÖ Scraping completed successfully!

PS D:\MLOPS\SentimentProjek> python src/preprocessing/preprocess.py

üîÑ Starting preprocessing pipeline...
=====================================

üìÇ Loading data...
File: data/raw/reviews_raw_20251203.csv
Rows: 954

üßπ Cleaning text...
‚úì Removed HTML tags
‚úì Removed URLs
‚úì Converted to lowercase
‚úì Removed special characters
‚úì Removed stopwords (ID + EN)

üìä Labeling sentiment...
‚úì Rule-based labeling (rating threshold)

üìà Feature engineering...
‚úì Text length calculated
‚úì Word count calculated

üíæ Saving processed data...
‚úì Connected to PostgreSQL
‚úì Saved to table: processed_reviews
‚úì Saved to CSV: data/processed/processed_reviews_20251203.csv

üìä Final Statistics:
- Positive: 774 (81.1%)
- Negative: 137 (14.3%)
- Neutral: 43 (4.6%)
- Avg Rating: 4.3 ‚≠ê
- Avg Text Length: 127 chars
- Avg Word Count: 18 words

‚è±Ô∏è Duration: 12.8 seconds
‚úÖ Preprocessing completed successfully!
```

---

**END OF REPORT**
