name: ML CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.10'

jobs:
  # Job 1: Data Validation
  data-validation:
    name: Data Validation
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pandas numpy scikit-learn
          pip install -r requirements.txt
      
      - name: Run data validation tests
        run: |
          pytest tests/test_data_validation.py -v --tb=short
        continue-on-error: true
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-validation-results
          path: |
            pytest-report.xml
            .pytest_cache/

  # Job 2: Model Validation
  model-validation:
    name: Model Validation
    runs-on: ubuntu-latest
    needs: data-validation
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest torch transformers
          pip install -r requirements.txt
      
      - name: Download model artifacts (if available)
        continue-on-error: true
        run: |
          echo "Checking for model artifacts..."
          # In production, download from model registry or S3
      
      - name: Run model validation tests
        run: |
          pytest tests/test_model_validation.py -v --tb=short
        continue-on-error: true
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: model-validation-results
          path: |
            pytest-report.xml

  # Job 3: Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [data-validation, model-validation]
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    env:
      POSTGRES_HOST: localhost
      POSTGRES_PORT: 5432
      POSTGRES_DB: test_db
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest psycopg2-binary
          pip install -r requirements.txt
      
      - name: Setup test database
        run: |
          PGPASSWORD=$POSTGRES_PASSWORD psql -h localhost -U $POSTGRES_USER -d $POSTGRES_DB -c "
          CREATE TABLE IF NOT EXISTS reviews (
            id SERIAL PRIMARY KEY,
            review_text TEXT,
            sentiment VARCHAR(50),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );
          
          CREATE TABLE IF NOT EXISTS model_metrics (
            id SERIAL PRIMARY KEY,
            model_name VARCHAR(100),
            accuracy FLOAT,
            precision_score FLOAT,
            recall_score FLOAT,
            f1_score FLOAT,
            train_accuracy FLOAT,
            train_precision FLOAT,
            train_recall FLOAT,
            train_f1 FLOAT,
            dataset_type VARCHAR(50),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );
          "
      
      - name: Run integration tests
        run: |
          pytest tests/test_integration.py -v --tb=short
        continue-on-error: true
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            pytest-report.xml

  # Job 4: Code Quality & Security
  code-quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pylint black isort bandit safety
      
      - name: Run Black (code formatting check)
        run: |
          black --check --diff src/ tests/ || true
      
      - name: Run isort (import sorting check)
        run: |
          isort --check-only --diff src/ tests/ || true
      
      - name: Run flake8 (linting)
        run: |
          flake8 src/ tests/ --max-line-length=120 --ignore=E501,W503 || true
      
      - name: Run Bandit (security check)
        run: |
          bandit -r src/ -ll || true
      
      - name: Check dependencies for vulnerabilities
        run: |
          pip install -r requirements.txt
          safety check --json || true

  # Job 5: Model Training (on main branch only)
  model-training:
    name: Model Training & Registry
    runs-on: ubuntu-latest
    needs: [integration-tests, code-quality]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download training data
        run: |
          echo "Downloading training data..."
          # In production, download from DVC or S3
      
      - name: Run model training
        run: |
          echo "Training model..."
          # python src/training/train_bert.py
          echo "Training skipped in CI (too resource intensive)"
      
      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        if: success()
        with:
          name: trained-model
          path: |
            models/
            mlruns/
      
      - name: Push to model registry
        run: |
          echo "Pushing model to registry..."
          # In production, push to MLflow registry or S3

  # Job 6: Generate Test Report
  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [data-validation, model-validation, integration-tests, code-quality]
    if: always()
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download all test results
        uses: actions/download-artifact@v4
      
      - name: Generate summary report
        run: |
          echo "# ML Pipeline Test Summary" > test_summary.md
          echo "" >> test_summary.md
          echo "**Date:** $(date)" >> test_summary.md
          echo "**Branch:** ${{ github.ref }}" >> test_summary.md
          echo "**Commit:** ${{ github.sha }}" >> test_summary.md
          echo "" >> test_summary.md
          echo "## Test Results" >> test_summary.md
          echo "- Data Validation: ${{ needs.data-validation.result }}" >> test_summary.md
          echo "- Model Validation: ${{ needs.model-validation.result }}" >> test_summary.md
          echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> test_summary.md
          echo "- Code Quality: ${{ needs.code-quality.result }}" >> test_summary.md
      
      - name: Upload summary report
        uses: actions/upload-artifact@v4
        with:
          name: test-summary-report
          path: test_summary.md
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('test_summary.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
