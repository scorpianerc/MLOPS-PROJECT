# DVC Integration & GitHub Actions - Setup Guide

## ‚úÖ DVC (Data Version Control) Setup Complete

### 1. **DVC Initialization**
```bash
dvc init
dvc remote add -d localstorage D:\MLOPS\dvc-storage
```

### 2. **DVC Pipeline Stages**
- ‚úÖ `data_collection` - Scrape reviews from Google Play Store
- ‚úÖ `preprocessing` - Clean and prepare data for training
- ‚úÖ `training_bert` - Train IndoBERT model (85%+ accuracy)
- ‚úÖ `training_traditional` - Train Logistic Regression/Random Forest

### 3. **Tracked Files**
```
data/raw/reviews.csv              (3937 reviews)
data/processed/processed_reviews.csv
models/bert_model/                (IndoBERT fine-tuned)
models/sentiment_model.pkl        (Traditional ML)
models/vectorizer.pkl
models/metrics.json
models/bert_metrics.json
```

### 4. **DVC Commands**
```bash
# Check status
dvc status

# Reproduce pipeline (run all stages)
dvc repro

# Run specific stage
dvc repro training_bert

# Show metrics
dvc metrics show

# Compare metrics between branches
dvc metrics diff main

# Push to remote storage
dvc push

# Pull from remote storage
dvc pull
```

---

## ‚úÖ GitHub Actions Workflows Complete

### 1. **DVC Pipeline Workflow** (`.github/workflows/dvc-pipeline.yml`)
**Triggers:**
- Push to `main`, `master`, `develop` branches
- Changes in `data/`, `src/`, `dvc.yaml`, `params.yaml`
- Manual workflow dispatch

**Steps:**
- ‚úÖ Pull DVC data from remote
- ‚úÖ Run data collection (if needed)
- ‚úÖ Run preprocessing
- ‚úÖ Train BERT model
- ‚úÖ Train traditional ML models
- ‚úÖ Show metrics and plots
- ‚úÖ Compare metrics with main branch (PR only)
- ‚úÖ Push DVC outputs to remote
- ‚úÖ Upload metrics and plots as artifacts
- ‚úÖ Comment PR with model performance

### 2. **Model Training Workflow** (`.github/workflows/model-training.yml`)
**Triggers:**
- Push to branches with changes in `src/training/`, `src/preprocessing/`, `params.yaml`
- Manual dispatch with model type selection (BERT/Traditional/Both)

**Steps:**
- ‚úÖ Setup Python environment
- ‚úÖ Install dependencies and NLTK data
- ‚úÖ Pull DVC data
- ‚úÖ Train selected model(s)
- ‚úÖ Validate models and metrics
- ‚úÖ Upload model artifacts (retained 30 days)
- ‚úÖ Create training summary in GitHub
- ‚úÖ Compare with baseline (PR only)

### 3. **Docker Build & Deploy Workflow** (`.github/workflows/docker-deploy.yml`)
**Triggers:**
- Push to `main`/`master` branches
- Git tags (`v*`)
- Manual dispatch

**Steps:**
- ‚úÖ Build Docker image with Buildx
- ‚úÖ Push to GitHub Container Registry (ghcr.io)
- ‚úÖ Tag with branch/version/SHA
- ‚úÖ Deploy to staging (develop branch)
- ‚úÖ Deploy to production (main/master branch)
- ‚úÖ Health checks

---

## üîß Configuration Required

### **GitHub Secrets** (Repository Settings ‚Üí Secrets)
Add these secrets for full functionality:

```
DVC_REMOTE_URL           # Optional: S3/GCS bucket URL for DVC remote
POSTGRES_USER            # Database username
POSTGRES_PASSWORD        # Database password
POSTGRES_DB              # Database name
MONGO_DB                 # MongoDB database name
GRAFANA_ADMIN_USER       # Grafana admin username
GRAFANA_ADMIN_PASSWORD   # Grafana admin password
```

### **DVC Remote Storage Options**

**Option 1: Local Storage (Current)**
```bash
dvc remote add -d localstorage D:\MLOPS\dvc-storage
```

**Option 2: AWS S3**
```bash
dvc remote add -d s3remote s3://mybucket/dvcstore
dvc remote modify s3remote access_key_id YOUR_ACCESS_KEY
dvc remote modify s3remote secret_access_key YOUR_SECRET_KEY
```

**Option 3: Google Cloud Storage**
```bash
dvc remote add -d gcsremote gs://mybucket/dvcstore
# Configure GCS credentials
```

**Option 4: Azure Blob Storage**
```bash
dvc remote add -d azureremote azure://mycontainer/dvcstore
```

**Option 5: GitHub (Git LFS)**
```bash
# Add to .github/workflows - store in artifacts
```

---

## üìä Usage Examples

### **1. Train Model Locally with DVC**
```bash
# Run entire pipeline
dvc repro

# Run specific stage
dvc repro training_bert

# Show results
dvc metrics show
dvc plots show
```

### **2. Trigger GitHub Actions Manually**
1. Go to **Actions** tab in GitHub
2. Select workflow: "Model Training & Testing"
3. Click **Run workflow**
4. Choose model type: BERT / Traditional / Both
5. Click **Run workflow**

### **3. Create New Branch with Model Experiment**
```bash
git checkout -b experiment/new-model
# Modify params.yaml
git add params.yaml
git commit -m "Experiment: new hyperparameters"
git push origin experiment/new-model
# Create PR ‚Üí GitHub Actions will compare metrics automatically
```

### **4. Deploy to Production**
```bash
git checkout main
git merge develop
git tag v1.0.0
git push origin main --tags
# Docker build & deploy workflow runs automatically
```

---

## üîç Monitoring & Validation

### **Check DVC Pipeline Status**
```bash
# Show tracked files
dvc status

# Validate pipeline
dvc dag

# Show dependencies graph
dvc dag --md > pipeline.md
```

### **Check GitHub Actions**
- Go to repository **Actions** tab
- View workflow runs, logs, and artifacts
- Download model artifacts and metrics

### **View Metrics in PR**
- Create PR ‚Üí Bot automatically comments with model performance
- Compare metrics between branches
- Review plots and confusion matrices

---

## üì¶ Files Created/Modified

### **New Files:**
- `.github/workflows/dvc-pipeline.yml` - DVC pipeline automation
- `.github/workflows/model-training.yml` - Model training workflow
- `.github/workflows/docker-deploy.yml` - Docker build & deploy
- `dvc.lock` - DVC lock file (tracks file hashes)
- `.dvc/config` - DVC configuration
- `*.dvc` files - Metadata for tracked files

### **Modified Files:**
- `dvc.yaml` - Updated with BERT training stage
- `params.yaml` - Fixed duplicate `bert_model` key
- `.gitignore` - Added DVC tracked files

---

## üéØ Next Steps

1. **Setup DVC Remote Storage**
   - Choose cloud provider (AWS S3 / GCS / Azure)
   - Add credentials to GitHub Secrets
   - Update `.dvc/config` with remote URL

2. **Test GitHub Actions**
   - Make a small change and push
   - Verify workflow runs successfully
   - Check artifacts and metrics

3. **Configure Production Deployment**
   - Setup production server
   - Add deployment credentials to secrets
   - Uncomment deployment commands in workflow

4. **Setup Monitoring**
   - Configure Grafana dashboards
   - Setup alerts for model performance
   - Monitor DVC pipeline execution

---

## üêõ Troubleshooting

### **DVC Issues**
```bash
# Reset DVC
dvc checkout --force

# Clean cache
dvc gc -w

# Verify remote
dvc remote list
dvc push -v
```

### **GitHub Actions Issues**
- Check workflow logs in Actions tab
- Verify secrets are configured
- Test locally: `act -l` (using nektos/act)

### **Model Training Issues**
```bash
# Test locally first
python src/preprocessing/preprocess.py
python src/training/train_bert.py

# Check data
ls -lah data/processed/
head data/processed/processed_reviews.csv
```

---

## ‚ú® Summary

‚úÖ **DVC initialized** with local storage  
‚úÖ **Pipeline configured** with 4 stages (collection, preprocessing, bert, traditional)  
‚úÖ **GitHub Actions created** for DVC, training, and deployment  
‚úÖ **Files tracked** (3937 reviews, BERT model 498MB, metrics)  
‚úÖ **Remote storage ready** for cloud backup  
‚úÖ **CI/CD pipeline** for automated training and deployment  

Your MLOps project is now fully integrated with DVC and GitHub Actions! üöÄ
